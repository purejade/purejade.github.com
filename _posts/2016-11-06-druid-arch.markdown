---
layout: post
title:  "Druid系列翻译与实践——Druid架构"
date:   2016-11-06 10:47:29 +0800
categories: druid
---

如果希望对Druid架构有个全面的了解，建议阅读[White Paper](http://static.druid.io/docs/druid.pdf)，但由于目前Druid还在不断开发中，因此Paper可能是滞后的，因此建议查看官网文档。

## Druid 是什么？

Druid 是允许对大量几乎不可变数据集合进行快速访问(real-time)的系统，它设计的意图是作为一个服务，在面对部署、机器故障以及其它生产环境可能情况都可以保持100%的正常运行。它也可以作为后台用例，但是设计决策明确地针对一个永远在线的服务。

Druid目前允许以类似Dremel和PowerDrill的方式进行单表查询。它增加了以下功能：

1. 添加支持部分嵌套数据结构的列式存储格式
2. 添加具有中间修剪功能的分层查询分布
3. 添加支持快速过滤的索引
4. 实时提取（所获取的数据可立即用于查询）
5. 容错分布式体系结构，不会丢失数据



就比较的系统而言，Druid 功能定位于位于PowerDrill和Dremel之间。它实现了几乎所有的Dremel提供的功能（除了，Dremel可以处理任意嵌套数据结构，而Druid只提供单一级别的基于数组的嵌套的处理），并从PowerDrill汲取了一些有趣的数据分布和压缩方法。


Druid 非常适合作为单个大数据流实时数据获取的产品。特别是如果您的目标是无当机操作，或者希望在传入数据流上基于时间的汇总之上构建您的产品。当谈到查询速度时，重要的是说清楚“快速”意味着：Druid 它在可能性的领域（我们做到了）实现在少于一秒钟运行数万亿行数据的查询。

## 架构

Druid 是一组系统，每个子系统作为独立角色，它们一起构成一个工作系统。它的名字来源于多角色扮演的游戏Druid：它是一个变形器，可以变换多种形式来适应一个组中不同角色。

下面描述的每个系统或组件都有一个更多细节的链接页面。您可以在右侧菜单中找到该页面，或单击组件描述中的链接。

包含以下组件：

#### Historical Node (历史节点) 

历史节点主要用来处理对“历史”数据（非实时）的存储和查询。历史节点从深度存储(deep storage)下载段，响应来自代理节点(broker)关于这些段的查询，并将结果返回到代理节点。它们保持和Zookeeper服务沟通，让Zookeeper来维护段信息。

#### Coordinator Node(协调节点)

协调器节点负责历史节点分组的监视，以确保数据可用，并采取备份策略来确保数据保持“最佳”配置。它们通过从元数据存储中读取段元数据信息来确定应在集群中加载哪些段，使用Zookeeper确定历史节点存在以及创建Zookeeper条目以告知历史节点加载和删除新段。

#### Broker nodes(代理节点)

代理节点从外部客户端接收查询，并将这些查询转发到实时和历史节点。当Broker节点接收结果时，它们合并这些结果并将它们返回给调用者。对于已知拓扑结构，Broker节点通过Zookeeper来判断实时和历史节点的存在。

#### Indexing Service nodes （索引节点）

索引服务节点形成一个工作集群，将批处理和实时数据加载到系统中，并允许对存储在系统中的数据进行更改。

#### Realtime nodes（实时节点）

实时节点也可以将实时数据加载到系统中。它们比索引服务更容易设置，以生产使用的一些限制为代价。

多个节点的组合可以达到物尽其用，事半功倍的效果。通过分离历史节点和实时处理节点，我们可以在对实时数据流进行监听并处理它以进入系统的时候，不用考虑对内存的关注（批处理对内存要求要）。通过分离协调器节点和代理节点，我们将查询的需求与维持集群中保持“良好”数据分布的需求分开。

下图显示了查询和数据在此体系结构中的位置，以及此架构中涉及到的节点以及外部依赖：

![http://druid.io/docs/img/druid-dataflow-3.png](http://druid.io/docs/img/druid-dataflow-3.png)





## 段和数据存储（Segments and Data Storage）

如上图所示，在数据导入Druid系统前需要一个索引过程，这使系统可以分析数据、添加索引结构、压缩和调整数据布局，从而可以优化查询速度。下面是数据流发生的列表：

- 转换为列式结构
- 用位图检索索引
- 使用各种压缩算法
  - LZ4 压缩列
    - 利用字典编码w / id存储最小化为字符串列
  - 位图索引的位图压缩

索引过程的输出称为“段”。段是 Druid 中存储数据的基本结构。“段”包含数据集中各种维度(dimensions)和指标(metrics)，以列方向存储，并包含这些列的索引。

段存储在“深存储”LOB存储/文件系统中（有关可能的选项信息，请参阅深存储。在历史节点加载数据时，首先将数据下载到其本地磁盘，然后利用内存映射机制提供查询。

如果历史节点死亡，它将不再服务其段，但是考虑到段仍然在“深存储”上可用，任何其他节点可以简单地下载该段并开始服务它。这意味着可以从集群中实际删除所有历史节点，然后再重新配置它们，而不会有任何数据丢失。这也意味着如果“深存储”不可用，则节点可以继续服务它们已经下拉的分段（即，集群变得陈旧，而不是宕机）。

为了使段在集群内部存在，必须将一个条目添加到元数据存储实例中的表。此条目是关于段的元数据的描述信息，包括段的模式、大小以及在深存储上的位置。这些条目用于协调器来了解集群上提供哪些数据的条目。



## 容错（Fault Tolerance）

历史节点，如上所述，如果历史节点死亡，另一个历史节点可以取代它的位置，并且不担心数据丢失。

协调器节点可以在热故障切换配置中运行。如果没有协调器在运行，则无法对数据拓扑进行更改（无新数据和无数据平衡决策），但系统将继续运行。

代理节点可以并行运行或热故障切换。

索引服务节点通过复制摄取任务的方式运行，协调节点可以采取热故障转移。

实时节点依赖传送流的语义，可以并行启动多个来处理完全相同的流。他们定期把数据放到磁盘，最终PUSH到深存储。可以采取措施从进程死亡中恢复，但是如果这是向系统添加数据的唯一方法，则丢失对本地磁盘的访问可能导致数据丢失。

“深存储”文件系统如果这不可用，则新数据将无法进入集群，但集群将继续按原样运行。

元数据存储如果不可用，协调器节点将无法找到系统中的新段，但它将继续以之前视图信息应对存在于集群中的段。

ZooKeeper如果不可用，则无法更新数据拓扑，但Brokers将保留最近的数据拓扑视图，并相应地继续提供请求。



## 查询处理 （Query Processing）

查询首先进入到代理节点，其中代理将匹配查询与已知存在的数据段。然后，它将选择一组服务于这些段的机器，并重写查询到包含这些段的指定服务器。历史/实时进程将接收查询，处理它们并返回结果。 Broker然后获取结果并将它们合并在一起以获得最终答案，然后返回。这样，代理可以在查看单个数据行之前过滤掉与查询不匹配的所有数据。

对于比Broker可以修剪的更细粒度的过滤器，每个段内的索引结构允许历史节点在查看任何数据行之前，确定哪些（如果有的话）行与过滤器集匹配。它并不需要查看每一行数据，只需要对位图索引执行过滤器的所有布尔代数。

一旦它得到与当前查询匹配的行，它就可以直接访问它关心的那些列，而不必加载那些无相关数据。

## 都在内存？（In-memory?）

Druid 并不总是在内存中。当我们第一次构建它，它真的是一直在内存里，但随着时间的推移，对于价格-性能的权衡结束了将所有的客户数据保存在内存中。然后，我们添加了内存映射数据的能力，并允许操作系统根据需要需求将分页数据换入换出内存。我们生产集群的主要配置为了使用这种内存映射行为，并且我们肯定在可用内存与节点服务的数据方面过度配置了。

当你阅读一些旧的博客文章或其他关于该项目的文献时，你会看到“内存中”经常被鼓吹，因为这是Druid历史的缘故，但技术现实是，存在一个价格与性能的调整空间。它可以从所有内存（高成本，高性能）到大多数磁盘（低成本，低性能）自由配置来平衡价格和性能。



> 官网架构 http://druid.io/docs/0.9.1.1/design/design.html